# -*- coding: utf-8 -*-
"""arxiv_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rHNmNIFRAiLMNhX-Zz07NcPa2CJetnDF

**Trying Hugging Face Tutt with Arxiv Dataset**

Starting with data prep
"""

#!pip install transformers
from transformers import pipeline

!pip install kaggle rouge_score datasets

import os
os.environ['KAGGLE_CONFIG_DIR'] = "/content"

!kaggle datasets download -d Cornell-University/arxiv
!chmod 600 /content/kaggle.json

!unzip /content/arxiv.zip -d arxiv-dataset

from datasets import load_dataset
dataset = load_dataset("arxiv_dataset", data_dir='./arxiv-dataset/', split='train', ignore_verifications=True)

dataset = dataset.shuffle(seed=42)
dataset = dataset.select(range(25000))

import pandas as pd
df = pd.DataFrame(dataset)

#only keep columns that are required
df = df[['abstract', 'title']]
df = df.rename(columns={"abstract": "text", "title": "summary"})

df = df.replace(r'\n',' ', regex=True)
pd.options.display.max_colwidth = 100

cutoff_summary = 5
cutoff_text = 20
df = df[(df['summary'].apply(lambda x: len(x.split()) >= cutoff_summary)) & (df['text'].apply(lambda x: len(x.split()) >= cutoff_text))]

df = df.sample(1000, random_state=43)
import numpy as np

# split the dataset into train, val, and test
df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=44), [int(0.8*len(df)), int((0.9)*len(df))])

"""Data now split -- needs to be tokenized"""

from transformers import AutoTokenizer

checkpoint = "t5-small"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

prefix = "summarize: "


def preprocess_function(examples):
    inputs = [prefix + doc for doc in examples["text"]]
    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)

    labels = tokenizer(text_target=examples["summary"], max_length=128, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

import pyarrow as pa
from datasets import Dataset

# convert to Huggingface dataset
ds_train = Dataset(pa.Table.from_pandas(df_train))
tokenized_arxiv_train = ds_train.map(preprocess_function, batched=True)

ds_test = Dataset(pa.Table.from_pandas(df_test))
tokenized_arxiv_test = ds_test.map(preprocess_function, batched=True)

from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)

"""HuggingFace Evaluation"""

#!pip install evaluate
import evaluate
rouge = evaluate.load("rouge")

import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)

    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
    result["gen_len"] = np.mean(prediction_lens)

    return {k: round(v, 4) for k, v in result.items()}

"""HuggingFace Train"""

from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

!pip install accelerate -U
!pip install transformers[torch]

from huggingface_hub import notebook_login

notebook_login()

import accelerate

training_args = Seq2SeqTrainingArguments(
    output_dir="arxiv_model",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=2,
    predict_with_generate=True,
    push_to_hub=True,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_arxiv_train,
    eval_dataset=tokenized_arxiv_test,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.push_to_hub()

# from transformers import pipeline

# text = "summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes."
# summarizer = pipeline("summarization", model="scural/arxiv_model")
# summarizer(text)

# text = "summarize: We discuss the current state of the art of Quantum Random Number Generators (QRNG) and their possible applications in the search for quantum advantages. To this aim, we first discuss a possible way of benchmarking QRNG by applying them to the computation of complicated and hard to realize classical simulations, such as critical dynamics in two-dimensional Ising lattices. These are performed with the help of computing devices based on field-programmable gate arrays (FPGAs) or graphic processing units (GPUs). The results obtained for QRNG are compared with those obtained by classical pseudo-random number generators (PRNG) of various qualities. Monte Carlo simulations of critical dynamics in moderate lattice sizes (128Ã—128) start to be sensitive to the correlations present in pseudo-random numbers sequences, allowing us to detect them."
# summarizer(text)

